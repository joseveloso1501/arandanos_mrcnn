{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9csYdOiCeux-"
   },
   "source": [
    "DIRECTORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1659596004828,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "81FEez0ZAakw",
    "outputId": "2a36cd22-cf9f-42b2-a37f-3cce274d7c1c"
   },
   "outputs": [],
   "source": [
    "#cd '/tf/PT_JoseVeloso/Mask_RCNN-master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/PT_JoseVeloso/Mask_RCNN-master_matterport/model-training'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/PT_JoseVeloso/Mask_RCNN-master_matterport\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE                   \u001b[0m\u001b[01;34mdamage_cfg20220812T0733\u001b[0m/  \u001b[01;34mlogs\u001b[0m/\r\n",
      "MANIFEST.in               \u001b[01;34mdamage_cfg20220812T0734\u001b[0m/  \u001b[01;34mmask_rcnn.egg-info\u001b[0m/\r\n",
      "README.md                 \u001b[01;34mdamage_cfg20220812T0738\u001b[0m/  mask_rcnn_balloon.h5\r\n",
      "\u001b[01;34mapp\u001b[0m/                      \u001b[01;34mdamage_cfg20220812T0739\u001b[0m/  mask_rcnn_coco.h5\r\n",
      "\u001b[01;34massets\u001b[0m/                   \u001b[01;34mdamage_cfg20220812T0819\u001b[0m/  \u001b[01;34mmodel\u001b[0m/\r\n",
      "\u001b[01;34mbuild\u001b[0m/                    \u001b[01;34mdamage_cfg20220817T0537\u001b[0m/  \u001b[01;34mmodel-training\u001b[0m/\r\n",
      "\u001b[01;34mcustomImages\u001b[0m/             \u001b[01;34mdamage_cfg20220817T0538\u001b[0m/  \u001b[01;34mmrcnn\u001b[0m/\r\n",
      "\u001b[01;34mdamage_cfg20220802T0650\u001b[0m/  \u001b[01;34mdamage_cfg20220817T0541\u001b[0m/  report.txt\r\n",
      "\u001b[01;34mdamage_cfg20220802T1832\u001b[0m/  \u001b[01;34mdamage_cfg20220817T0622\u001b[0m/  requirements.txt\r\n",
      "\u001b[01;34mdamage_cfg20220804T0653\u001b[0m/  \u001b[01;34mdatasets\u001b[0m/                 \u001b[01;34msamples\u001b[0m/\r\n",
      "\u001b[01;34mdamage_cfg20220804T0658\u001b[0m/  \u001b[01;34mdist\u001b[0m/                     setup.cfg\r\n",
      "\u001b[01;34mdamage_cfg20220812T0652\u001b[0m/  \u001b[01;34mimages\u001b[0m/                   setup.py\r\n",
      "\u001b[01;34mdamage_cfg20220812T0654\u001b[0m/  instalacion\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6qajN7v0saE"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6812,
     "status": "ok",
     "timestamp": 1659596011947,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "iyA_0ghC0saJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "#sys.path.append(\"/tf/PT_JoseVeloso/Mask_RCNN-master/\")\n",
    "\n",
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug\n",
    "\n",
    "# import mask rcnn libraries\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize\n",
    "\n",
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "\n",
    "# import keras libraries\n",
    "from tensorflow.keras.preprocessing.image import load_img   #keras.preprocessing.image tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1659596011947,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "UbiJowrm0saL"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mbj3-t70saL"
   },
   "source": [
    "# Stage-1 Training (Single Class & Bounding Box Annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGtz-ovS0saM"
   },
   "source": [
    "In Stage-1 training I used a simple dataset with images annotated using Bounding Boxes, and one class namely 'Damage'. In the following section you can find the code for training of the model. I have included comments to best desccribe the flow of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3787125,
     "status": "ok",
     "timestamp": 1659604139130,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "dF77APDH0saM",
    "outputId": "aa2a0330-e201-4b00-b12c-fee7986449cb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id:  522\n",
      "image_id:  472\n",
      "image_id:  556\n",
      "image_id:  461\n",
      "image_id:  442\n",
      "image_id:  499\n",
      "image_id:  514\n",
      "image_id:  505\n",
      "image_id:  421\n",
      "image_id:  455\n",
      "image_id:  587\n",
      "image_id:  447\n",
      "image_id:  504\n",
      "image_id:  468\n",
      "image_id:  434\n",
      "image_id:  567\n",
      "image_id:  561\n",
      "image_id:  518\n",
      "image_id:  596\n",
      "image_id:  571\n",
      "image_id:  560\n",
      "image_id:  530\n",
      "image_id:  562\n",
      "image_id:  574\n",
      "image_id:  515\n",
      "image_id:  570\n",
      "image_id:  594\n",
      "image_id:  507\n",
      "image_id:  493\n",
      "image_id:  564\n",
      "image_id:  529\n",
      "image_id:  464\n",
      "image_id:  536\n",
      "image_id:  466\n",
      "image_id:  575\n",
      "image_id:  496\n",
      "image_id:  444\n",
      "image_id:  454\n",
      "image_id:  478\n",
      "image_id:  519\n",
      "image_id:  533\n",
      "image_id:  501\n",
      "image_id:  438\n",
      "image_id:  581\n",
      "image_id:  448\n",
      "image_id:  502\n",
      "image_id:  547\n",
      "image_id:  583\n",
      "image_id:  446\n",
      "image_id:  436\n",
      "image_id:  535\n",
      "image_id:  548\n",
      "image_id:  549\n",
      "image_id:  558\n",
      "image_id:  550\n",
      "image_id:  451\n",
      "image_id:  477\n",
      "image_id:  543\n",
      "image_id:  420\n",
      "image_id:  572\n",
      "image_id:  528\n",
      "image_id:  590\n",
      "image_id:  526\n",
      "image_id:  576\n",
      "image_id:  490\n",
      "image_id:  440\n",
      "image_id:  480\n",
      "image_id:  538\n",
      "image_id:  484\n",
      "image_id:  584\n",
      "image_id:  580\n",
      "image_id:  520\n",
      "image_id:  456\n",
      "image_id:  525\n",
      "image_id:  544\n",
      "image_id:  452\n",
      "image_id:  453\n",
      "image_id:  537\n",
      "image_id:  593\n",
      "image_id:  435\n",
      "image_id:  463\n",
      "image_id:  555\n",
      "image_id:  540\n",
      "image_id:  432\n",
      "image_id:  565\n",
      "image_id:  465\n",
      "image_id:  441\n",
      "image_id:  527\n",
      "image_id:  513\n",
      "image_id:  554\n",
      "image_id:  509\n",
      "image_id:  508\n",
      "image_id:  542\n",
      "image_id:  426\n",
      "image_id:  457\n",
      "image_id:  437\n",
      "image_id:  445\n",
      "image_id:  589\n",
      "image_id:  569\n",
      "image_id:  541\n",
      "image_id:  568\n",
      "image_id:  430\n",
      "image_id:  469\n",
      "image_id:  557\n",
      "image_id:  503\n",
      "image_id:  585\n",
      "image_id:  595\n",
      "image_id:  471\n",
      "image_id:  582\n",
      "image_id:  588\n",
      "image_id:  559\n",
      "image_id:  479\n",
      "image_id:  439\n",
      "image_id:  486\n",
      "image_id:  425\n",
      "image_id:  579\n",
      "image_id:  470\n",
      "image_id:  586\n",
      "image_id:  498\n",
      "image_id:  427\n",
      "image_id:  488\n",
      "image_id:  467\n",
      "image_id:  546\n",
      "image_id:  473\n",
      "image_id:  534\n",
      "image_id:  492\n",
      "image_id:  523\n",
      "image_id:  483\n",
      "image_id:  517\n",
      "image_id:  443\n",
      "image_id:  591\n",
      "image_id:  481\n",
      "image_id:  566\n",
      "image_id:  510\n",
      "image_id:  494\n",
      "image_id:  524\n",
      "image_id:  506\n",
      "image_id:  600\n",
      "image_id:  476\n",
      "image_id:  433\n",
      "image_id:  491\n",
      "image_id:  532\n",
      "image_id:  598\n",
      "image_id:  450\n",
      "image_id:  592\n",
      "image_id:  516\n",
      "image_id:  573\n",
      "image_id:  545\n",
      "image_id:  497\n",
      "image_id:  487\n",
      "image_id:  512\n",
      "image_id:  563\n",
      "image_id:  551\n",
      "image_id:  495\n",
      "image_id:  552\n",
      "image_id:  531\n",
      "image_id:  485\n",
      "image_id:  458\n",
      "image_id:  475\n",
      "image_id:  553\n",
      "image_id:  482\n",
      "image_id:  599\n",
      "image_id:  489\n",
      "image_id:  577\n",
      "image_id:  449\n",
      "image_id:  500\n",
      "image_id:  597\n",
      "image_id:  539\n",
      "image_id:  578\n",
      "\n",
      "Starting at epoch 0. LR=0.002\n",
      "\n",
      "Checkpoint Path: ./damage_cfg20220817T2105/mask_rcnn_damage_cfg_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    214\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(weights_path, \n\u001b[1;32m    215\u001b[0m                    by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    216\u001b[0m                    exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrcnn_class_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrcnn_bbox_fc\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrcnn_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrcnn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# start the training of model\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# you can change epochs and layers (head or all)\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tf/PT_JoseVeloso/Mask_RCNN-master_matterport/model-training/mrcnn/model.py:2401\u001b[0m, in \u001b[0;36mMaskRCNN.train\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2399\u001b[0m     workers \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[0;32m-> 2401\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALIDATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, epochs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:1233\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \n\u001b[1;32m   1224\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m  this endpoint.\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:570\u001b[0m, in \u001b[0;36mGeneratorOrSequenceTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    567\u001b[0m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps_per_epoch, x)\n\u001b[1;32m    568\u001b[0m training_utils_v1\u001b[38;5;241m.\u001b[39mcheck_generator_arguments(\n\u001b[1;32m    569\u001b[0m     y, sample_weight, validation_split\u001b[38;5;241m=\u001b[39mvalidation_split)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:212\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m target_steps:\n\u001b[0;32m--> 212\u001b[0m   batch_data \u001b[38;5;241m=\u001b[39m \u001b[43m_get_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m batch_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataset:\n\u001b[1;32m    215\u001b[0m       \u001b[38;5;66;03m# The dataset passed by the user ran out of batches.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m       \u001b[38;5;66;03m# Now we know the cardinality of the dataset.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m       \u001b[38;5;66;03m# If steps_per_epoch was specified, then running out of data is\u001b[39;00m\n\u001b[1;32m    218\u001b[0m       \u001b[38;5;66;03m# unexpected, so we stop training and inform the user.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:346\u001b[0m, in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m   generator_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mStopIteration\u001b[39;00m, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError):\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py:891\u001b[0m, in \u001b[0;36mGeneratorEnqueuer.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 891\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mtask_done()\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DamageDataset(Dataset):\n",
    "    \n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # we add a class that we need to classify in our case it is Damage\n",
    "        \n",
    "        self.add_class(\"dataset\", 1, \"Damage\")\n",
    "        #self.add_class(\"dataset\", 1, \"arandano\")\n",
    "        \n",
    "        # we concatenate the dataset_dir with /images and /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            \n",
    "            # if is_train is True skip all images with id greater than and equal to 420\n",
    "            # roughly 80% of dataset for training\n",
    "            #frame0985\n",
    "            \n",
    "            #if is_train and int(image_id) >= 3835 :\n",
    "            if is_train and int(image_id) >= 420 :\n",
    "                print(\"image_id: \", image_id)\n",
    "                continue\n",
    "            \n",
    "            # if is_train is not True skip all images with id less than 420\n",
    "            #if not is_train and int(image_id) < 3835:\n",
    "            if not is_train and int(image_id) < 420:\n",
    "                continue\n",
    "            \n",
    "            # declaring image path and annotations path\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training or testing\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "        \n",
    "        # <annotation>\n",
    "        # <size>\n",
    "\n",
    "        #       <width>640</width>\n",
    "\n",
    "        #       <height>360</height>\n",
    "\n",
    "        #       <depth>3</depth>\n",
    "\n",
    "        # </size>\n",
    "\n",
    "\n",
    "        # <object>\n",
    "\n",
    "        #          <name>damage</name>\n",
    "\n",
    "        #          <pose>Unspecified</pose>\n",
    "\n",
    "        #          <truncated>0</truncated>\n",
    "\n",
    "        #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "        #          <bndbox>\n",
    "\n",
    "        #                 <xmin>315</xmin>\n",
    "\n",
    "        #                 <ymin>160</ymin>\n",
    "\n",
    "        #                 <xmax>381</xmax>\n",
    "\n",
    "        #                 <ymax>199</ymax>\n",
    "\n",
    "        #          </bndbox>\n",
    "\n",
    "        # </object>\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in image\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        # we append the class_id 1 for Damage in our case to the variable\n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000001110 \n",
    "        # 000011100 011100000 000001110\n",
    "        # 000011100 011100000 000001110\n",
    "        # 000000000 011100000 000000000\n",
    "        #    1         1          1    <- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('Damage'))\n",
    "            #class_ids.append(self.class_names.index('arandano'))\n",
    "        \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class DamageConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"damage_cfg\"\n",
    "    \n",
    "    # damage class + background class\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    #STEPS_PER_EPOCH = 361\n",
    "    STEPS_PER_EPOCH = 180\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    #VALIDATION_STEPS = 50\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    #Train_ROIs_Per_Image = 200\n",
    "    Train_ROIs_Per_Image = 100\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    #RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# load the train dataset\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset('customImages/stage-1', is_train=True)\n",
    "#train_set.load_dataset('customImages/IMG_train', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# load the test dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset('customImages/stage-1', is_train=False)\n",
    "#test_set.load_dataset('customImages/IMG_train', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# prepare config by calling the user defined confifuration class\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')\n",
    "\n",
    "#model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=3, layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvvcvGZp0saP"
   },
   "source": [
    "# Stage-2 (Mulptiple Class and Bounding Box Annotaions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlcTLUl-0saP"
   },
   "source": [
    "In the stage I trained the model on multiple classes namely level-1 (scratch), level-2 (dent), level-3 (shatter), and level-4 (dislocation). The images are annotated using bounding boxes.\n",
    "\n",
    "I recommend going through Stag-1 Training code first, there is not much difference between code of stage 1 and stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVXhfvHr0saQ"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1659596284470,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "i7CCL0vD0saQ"
   },
   "outputs": [],
   "source": [
    "class DamageDataset(Dataset):\n",
    "\n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # we use add_class for each class in our dataset and assign numbers to them. 0 is background\n",
    "        # self.add_class('source', 'class id', 'class name')\n",
    "        self.add_class(\"dataset\", 1, \"Level-1\")\n",
    "        self.add_class(\"dataset\", 2, \"Level-2\")\n",
    "        self.add_class(\"dataset\", 3, \"Level-3\")\n",
    "        self.add_class(\"dataset\", 4, \"Level-4\")\n",
    "        \n",
    "        # we concatenate the dataset_dir with /images and /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            # if is_train is True skip all images with id greater than and equal to 160\n",
    "             # roughly 80% of dataset for training\n",
    "            if is_train and int(image_id) >= 160 :\n",
    "                continue\n",
    "             \n",
    "            # if is_train is not True skip all images with id less than 420\n",
    "            if not is_train and int(image_id) < 160:\n",
    "                continue\n",
    "            \n",
    "            # img_path and ann_path variables are defined\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    \n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "\n",
    "        # <size>\n",
    "\n",
    "        #       <width>640</width>\n",
    "\n",
    "        #       <height>360</height>\n",
    "\n",
    "        #       <depth>3</depth>\n",
    "\n",
    "        # </size>\n",
    "\n",
    "\n",
    "        # <object>\n",
    "\n",
    "        #          <name>damage</name>\n",
    "\n",
    "        #          <pose>Unspecified</pose>\n",
    "\n",
    "        #          <truncated>0</truncated>\n",
    "\n",
    "        #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "        #          <bndbox>\n",
    "\n",
    "        #                 <xmin>315</xmin>\n",
    "\n",
    "        #                 <ymin>160</ymin>\n",
    "\n",
    "        #                 <xmax>381</xmax>\n",
    "\n",
    "        #                 <ymax>199</ymax>\n",
    "\n",
    "        #          </bndbox>\n",
    "\n",
    "        # </object>\n",
    "\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in an image\n",
    "        for box in root.findall('.//object'):\n",
    "            name = box.find('name').text\n",
    "            xmin = int(box.find('./bndbox/xmin').text)\n",
    "            ymin = int(box.find('./bndbox/ymin').text)\n",
    "            xmax = int(box.find('./bndbox/xmax').text)\n",
    "            ymax = int(box.find('./bndbox/ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax, name]\n",
    "            boxes.append(coors)\n",
    "            \n",
    "            # I have included this line to skip any un-annotated images\n",
    "            if name=='Level-1' or name=='Level-2' or name=='Level-3' or name=='Level-4':\n",
    "                boxes.append(coors)\n",
    "\n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances        \n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "\n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example: if 2.jpg have four objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000000000 022200000 000000000 000000000\n",
    "        #    1         2          3         1<- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            \n",
    "            # box[4] will have the name of the class for a particular damage\n",
    "            if (box[4] == 'Level-1'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "                class_ids.append(self.class_names.index('Level-1'))\n",
    "            elif(box[4] == 'Level-2'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 2\n",
    "                class_ids.append(self.class_names.index('Level-2')) \n",
    "            elif(box[4] == 'Level-3'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 3\n",
    "                class_ids.append(self.class_names.index('Level-3'))\n",
    "            else:\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 4\n",
    "                class_ids.append(self.class_names.index('Level-4'))\n",
    "                \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class DamageConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"damage_cfg\"\n",
    "    \n",
    "    #  background class + 4 classes\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 160\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# prepare train set\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset('customImages/stage-2', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# validation/test\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset('customImages/stage-2', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# load damage config\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, \n",
    "            test_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=5, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGh108FR0saU"
   },
   "source": [
    "# Stage 3 ( Multiple Classes and images annotated with polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbUWKXjt0saV"
   },
   "source": [
    "In this stage I have used multiple classes namely scratch, dent, shatter and dislocation. The images are annotated using a polygon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqfCiZIS0saV"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1659596284471,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "d46dSObQ0saV"
   },
   "outputs": [],
   "source": [
    "class DamageDataset(Dataset):\n",
    "\n",
    "    def load_dataset(self, dataset_dir, subset):\n",
    "        \n",
    "        # we use add_class for each class in our dataset and assign numbers to them. 0 is background\n",
    "        # self.add_class('source', 'class id', 'class name')\n",
    "        self.add_class(\"damage\", 1, \"Scratch\")\n",
    "        self.add_class(\"damage\", 2, \"Dent\")\n",
    "        self.add_class(\"damage\", 3, \"Shatter\")\n",
    "        self.add_class(\"damage\", 4, \"Dislocation\")\n",
    "        \n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        \n",
    "        # load annotations using json.load()\n",
    "        annotations1 = json.load(open(os.path.join(dataset_dir, \"via_project.json\")))\n",
    "        \n",
    "        # convert annotations1 into a list\n",
    "        annotations = list(annotations1.values())  \n",
    "        \n",
    "        # we only require the regions in the annotations\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            \n",
    "            # extracting shape attributes and region attributes\n",
    "            polygons = [r['shape_attributes'] for r in a['regions']] \n",
    "            objects = [s['region_attributes']['damage'] for s in a['regions']]\n",
    "            \n",
    "            # create a dictionary {name_of_class: class_id} remember background has id 0\n",
    "            name_dict = {\"Scratch\": 1, \"Dent\": 2, \"Shatter\": 3, \"Dislocation\": 4}\n",
    "            \n",
    "            # all the ids/classes in a image\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "            \n",
    "            # you can print these ids\n",
    "            # print(\"numids\",num_ids)\n",
    "            \n",
    "            # read image and get height and width\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "            \n",
    "            # add image to the dataset\n",
    "            self.add_image(\n",
    "                \"damage\",\n",
    "                image_id=a['filename'],\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "                )\n",
    "            \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # for cases when source is not damage\n",
    "        if info[\"source\"] != \"damage\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        \n",
    "        # get the class ids in an image\n",
    "        num_ids = info['num_ids']\n",
    "        \n",
    "        \n",
    "        \n",
    "        # we create len(info[\"polygons\"])(total number of polygons) number of masks of height 'h' and width 'w'\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        \n",
    "        # we loop over all polygons and generate masks (polygon mask) and class id for each instance\n",
    "        # masks can have any shape as we have used polygon for annotations\n",
    "        # for example: if 2.jpg have four objects we will have following masks and class_ids\n",
    "        # 000001100 000111000 000001110\n",
    "        # 000111100 011100000 000001110\n",
    "        # 000011111 011111000 000001110\n",
    "        # 000000000 111100000 000000000\n",
    "        #    1         2          3    <- class_ids\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "            mask[rr, cc, i] = 1\n",
    "            \n",
    "        # return masks and class_ids as array\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"damage\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n",
    "# define a configuration for the model\n",
    "class DamageConfig(Config):\n",
    "    # define the name of the configuration\n",
    "    NAME = \"damage\"\n",
    "    \n",
    "    # number of classes (background + damge classes)\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    \n",
    "    # number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 160\n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# prepare train dataset.\n",
    "train_set = DamageDataset()\n",
    "# change the dataset \n",
    "train_set.load_dataset(\"customImages/stage-3\", \"train\")\n",
    "train_set.prepare()\n",
    "\n",
    "# prepare validation/test dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset(\"customImages/stage-3\", \"val\")\n",
    "test_set.prepare()\n",
    "\n",
    "# load damage config\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, \n",
    "            test_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=15, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWwgRDrW0saW"
   },
   "source": [
    "# Model Evaluation (Stage-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1659596284471,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "W8R-eJaM0saW"
   },
   "outputs": [],
   "source": [
    "# we define a prediction configuration \n",
    "class PredictionConfig(Config):\n",
    "    NAME = \"damage\"\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    DETECTION_MIN_CONFIDENCE = 0.85\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "\n",
    "# evaluate_model is used to calculate mean Average Precision of the model\n",
    "def evaluate_model(dataset, model, cfg):\n",
    "    APs = list()\n",
    "    for image_id in dataset.image_ids:\n",
    "\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
    "\n",
    "        scaled_image = mold_image(image, cfg)\n",
    "\n",
    "        sample = expand_dims(scaled_image, 0)\n",
    "\n",
    "        yhat = model.detect(sample, verbose=0)\n",
    "\n",
    "        r = yhat[0]\n",
    "\n",
    "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "\n",
    "        APs.append(AP)\n",
    "    maP = mean(APs)\n",
    "    return mAP\n",
    "\n",
    "# train dataset\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset(\"customImages/stage-3\", \"train\")\n",
    "train_set.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset(\"customImages/stage-3\", \"val\")\n",
    "test_set.prepare()\n",
    "\n",
    "# create config\n",
    "cfg = PredictionConfig()\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
    "# load model weights\n",
    "model.load_weights('your_trained_model_weights.h5', by_name=True)\n",
    "\n",
    "# evaluate model on train dataset\n",
    "train_mAP = evaluate_model(train_set, model, cfg)\n",
    "print(\"Train mAP: %.3f\" % train_mAP[0])\n",
    "\n",
    "# evaluate model on test dataset\n",
    "test_mAP = evaluate_model(test_set, model, cfg)\n",
    "print(\"Test mAP: %.3f\" % test_mAP[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6m3tRQh0saX"
   },
   "source": [
    "# Single Image Prediction (Stage-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1659596284472,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "7r7UZplu0saX"
   },
   "outputs": [],
   "source": [
    "# define a configuration for the model\n",
    "class PredictionConfig(Config):\n",
    "    # define the name of the configuration\n",
    "    NAME = \"damage\"\n",
    "    # number of classes (background + kangaroo)\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    # number of training steps per epoch\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1659596284472,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "o97SBeV-0saX"
   },
   "outputs": [],
   "source": [
    "# load prediction configuration\n",
    "cfg = PredictionConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
    "\n",
    "# load model weights\n",
    "model_path = 'your_trained_weights.h5'\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1659596284472,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "3_hTXdWh0saX"
   },
   "outputs": [],
   "source": [
    "image = load_img(\"image_path.jpg\")\n",
    "image = img_to_array(image)\n",
    "\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "class_names = ['BG', 'Scratch', 'Dent', 'Shatter', 'Dislocation']\n",
    "\n",
    "r = results[0]\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names,  r['scores'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "master.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
