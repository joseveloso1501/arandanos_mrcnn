{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 11 fase 3\n",
    "\n",
    "Entrenamiento de Mask R-CNN con dataset modificado para emular \"Test 11\"\n",
    "\n",
    "### Hiperparametros\n",
    "* **epoch = 300**\n",
    "    * steps x epoch = 61 (lotes de imagenes)\n",
    "    * batch = 5 (imágenes por lote)\n",
    "* optimizador = SGD\n",
    "* Funcion de perdida = SMOOTHL1LOSS\n",
    "* Metrica de evaluacion = mAP (IoU >= 0.5)\n",
    "* **Mini-mask shape: 28x28**\n",
    "* **RPN anchor scales: (8, 16, 32, 64, 128)**\n",
    "* Tasa de aprendizaje: 0.001\n",
    "* **imagenes = 305**\n",
    "    * entrenamiento 70% = 214\n",
    "    * validacion 20% = 61\n",
    "    * evaluacion 10% = 31\n",
    "* etiquetas = 9140\n",
    "* **resolucion = 1920 x 1080**\n",
    "* etiquetas = bounding box formato VOC XML\n",
    "* **numero de clases = 3 (arandano, arandano-maduro, arandano-semi-maduro)**\n",
    "* **data augmentation = true**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9csYdOiCeux-"
   },
   "source": [
    "## Comprobar directorio principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/PT_JoseVeloso/Mask_RCNN-master_matterport/model-training\r\n",
      "total 1956\r\n",
      "drwxr-xr-x 4 root root   4096 Aug  7 06:10 build\r\n",
      "drwxr-xr-x 2 root root   4096 Aug  7 06:10 dist\r\n",
      "drwxr-xr-x 2 root root   4096 Aug  8 03:15 mask_rcnn.egg-info\r\n",
      "drwxr-xr-x 4 root root   4096 Sep 18 19:33 mrcnn\r\n",
      "drwxr-xr-x 3 root root   4096 Sep 15 02:27 old\r\n",
      "-rw-r--r-- 1 root root  67789 Sep 16 23:55 test_11.ipynb\r\n",
      "-rw-r--r-- 1 root root  92388 Sep 23 06:47 test_11_2_fase_2_corregido.ipynb\r\n",
      "-rw-r--r-- 1 root root  88232 Sep 17 07:27 test_11_fase_2-Copy1.ipynb\r\n",
      "-rw-r--r-- 1 root root  67792 Sep 17 08:36 test_11_fase_2.ipynb\r\n",
      "-rw-r--r-- 1 root root  87782 Sep 21 20:06 test_11_fase_2_corregido.ipynb\r\n",
      "-rw-r--r-- 1 root root  25842 Oct  8 07:11 test_11_fase_3.ipynb\r\n",
      "-rw-r--r-- 1 root root 350337 Sep 15 00:38 test_5-fase_2.ipynb\r\n",
      "-rw-r--r-- 1 root root 163924 Sep 18 23:12 test_5_corregido.ipynb\r\n",
      "-rw-r--r-- 1 root root 350144 Sep 17 08:42 test_5_fase_2.ipynb\r\n",
      "-rw-r--r-- 1 root root 102855 Sep 20 18:56 test_5_fase_2_corregido.ipynb\r\n",
      "-rw-r--r-- 1 root root 111017 Sep 18 16:23 test_6.ipynb\r\n",
      "-rw-r--r-- 1 root root  80973 Sep 19 21:10 test_6_corregido.ipynb\r\n",
      "-rw-r--r-- 1 root root 270148 Sep 17 16:09 test_6_fase_2.ipynb\r\n",
      "-rw-r--r-- 1 root root  92916 Sep 21 08:07 test_6_fase_2_corregido.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!pwd && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6qajN7v0saE"
   },
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6812,
     "status": "ok",
     "timestamp": 1659596011947,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "iyA_0ghC0saJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bibliotecas basicas\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "#sys.path.append(\"/tf/PT_JoseVeloso/Mask_RCNN-master/\")\n",
    "\n",
    "# bibliotecas avanzadas \n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug\n",
    "\n",
    "# bibliotecas mask rcnn \n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize\n",
    "\n",
    "# biblioteca matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# bibliotecas numpy \n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "\n",
    "# bibliotecas keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img   #keras.preprocessing.image tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# ignorar alertas de elementos que seran descontinuados\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%matplotlib inline\n",
    "#plt.show()\n",
    "\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mbj3-t70saL"
   },
   "source": [
    "# Fase 2 - Entrenamiento con dos clases y etiquetas de Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGtz-ovS0saM"
   },
   "source": [
    "En este entremamiento se utiliza un conjunto de datos simple con imágenes etiquetadas con cuadros delimitadores y una clase llamada 'Daño'. En la siguiente sección se encuentra el código para el entrenamiento del modelo. Se incluyen comentarios para describir mejor el flujo del programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetArandanos(Dataset):\n",
    "    \n",
    "    # la funcion load_dataset es usada para cargar el dataset de entrenamiento y test\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # se agrega una clase que se necesita para clasificar, en este caso arandano\n",
    "        # self.add_class('source', 'class id', 'class name')\n",
    "        self.add_class(\"dataset\", 1, \"arandano\")\n",
    "        self.add_class(\"dataset\", 2, \"arandano-maduro\")\n",
    "        self.add_class(\"dataset\", 3, \"arandano-semi-maduro\")\n",
    "        #self.add_class(\"dataset\", 4, \"arandano-semi\")\n",
    "        \n",
    "        # se concatena dataset_dir con /images y /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train sera Verdadero si se esta entrenando el modelo y Falso si se esta testeando el modelo\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # se usa para omitir los últimos 4 caracteres: '.jpg' (en class_id.jpg)\n",
    "            \n",
    "            # si is_train es Verdadero se omiten todas las imágenes con id mayor que e iguales a \n",
    "            # aproximadamente el 70% del conjunto de datos es para entrenamiento\n",
    "            if is_train and int(image_id) >= 40720 :\n",
    "                #print(\"image_id: \", image_id)\n",
    "                continue\n",
    "             \n",
    "            # si is_train no es Verdadero se omiten todas las imágenes con id menores a\n",
    "            if not is_train and int(image_id) < 40720:\n",
    "                continue\n",
    "            \n",
    "            # se declara la ruta de la imagen y la ruta de las etiquetas \n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # usando la función add_image se pasan image_id, image_path y ann_path para que la \n",
    "            # imagen actual se agregue al conjunto de datos para entrenamiento o prueba\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # funcino usada para extraer bouding boxes desde archivos etiquetados \n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # se puede ver en las imágenes que estan etiquetadas, como se extraen los valores de ancho, alto y bndbox\n",
    "\n",
    "        # <size>\n",
    "\n",
    "        #       <width>640</width>\n",
    "\n",
    "        #       <height>360</height>\n",
    "\n",
    "        #       <depth>3</depth>\n",
    "\n",
    "        # </size>\n",
    "\n",
    "\n",
    "        # <object>\n",
    "\n",
    "        #          <name>damage</name>\n",
    "\n",
    "        #          <pose>Unspecified</pose>\n",
    "\n",
    "        #          <truncated>0</truncated>\n",
    "\n",
    "        #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "        #          <bndbox>\n",
    "\n",
    "        #                 <xmin>315</xmin>\n",
    "\n",
    "        #                 <ymin>160</ymin>\n",
    "\n",
    "        #                 <xmax>381</xmax>\n",
    "\n",
    "        #                 <ymax>199</ymax>\n",
    "\n",
    "        #          </bndbox>\n",
    "\n",
    "        # </object>\n",
    "\n",
    "        # </annotation>\n",
    "        \n",
    "        # para analizar los archivos .xml\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # para obtener la raíz del archivo xml\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # se agregan todas las coordenadas x, y en boxes para todas las instancias de un objeto\n",
    "        boxes = list()\n",
    "        \n",
    "        # se encuentran todos los atributos con el nombre bndbox que existan para cada ground truth en la imagen\n",
    "        for box in root.findall('.//object'):\n",
    "            name = box.find('name').text\n",
    "            xmin = int(box.find('./bndbox/xmin').text)\n",
    "            ymin = int(box.find('./bndbox/ymin').text)\n",
    "            xmax = int(box.find('./bndbox/xmax').text)\n",
    "            ymax = int(box.find('./bndbox/ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax, name]\n",
    "            boxes.append(coors)\n",
    "                        \n",
    "            #quita todas las imagenes no etiquetadas\n",
    "            if name=='arandano' or name=='arandano-maduro'or name=='arandano-semi-maduro':\n",
    "                boxes.append(coors)\n",
    "\n",
    "        # extraer ancho y alto de la imagen\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # retorna boxes-> list, width-> int y height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # esta función llama al método extract_boxes y se usa para cargar una máscara para cada instancia en una imagen \n",
    "    # devuelve una máscara booleana con las siguientes dimensiones ancho * alto * instancias\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info apunta al image_id actual \n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # se obtiene la ruta de anotación de image_id que es dataset_dir/annots/image_id.xml \n",
    "        path = info['annotation']\n",
    "        \n",
    "         # se llama al método extract_boxes (arriba) para obtener bndbox del archivo .xml\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # se crea una cantidad de len(boxes) de mascaras de alto 'h' y ancho 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "\n",
    "        class_ids = list()\n",
    "        \n",
    "        ## se recorren todos los boxes y generamos máscaras (máscara de bndbox) y class id para cada instancia\n",
    "        # las máscaras tendrán forma rectangular ya que hemos usado bndboxes para etiquetas\n",
    "        # por ejemplo: si 2.jpg tiene tres objetos, tendremos las siguientes máscaras y class_ids.\n",
    "        \n",
    "        # 000000000 000000000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000000000 022200000 000000000 000000000\n",
    "        #    1         2          3         1<- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            \n",
    "            # box[4] will have the name of the class for a particular damage\n",
    "            if (box[4] == 'arandano'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "                class_ids.append(self.class_names.index('arandano'))\n",
    "            elif(box[4] == 'arandano-maduro'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 2\n",
    "                class_ids.append(self.class_names.index('arandano-maduro')) \n",
    "            elif(box[4] == 'arandano-semi-maduro'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 3\n",
    "                class_ids.append(self.class_names.index('arandano-semi-maduro'))\n",
    "\n",
    "                \n",
    "        # retorna mascaras y class_ids como arreglo\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # esta funciones toma el image_id y retorna la ruta de la imagen \n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "DEVICE                         /gpu:0\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.8\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (28, 28)\n",
      "NAME                           arandano_cfg_test_11_fase_3_\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 1.5]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                61\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "Train_ROIs_Per_Image           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class ConfigArandanos(Config):\n",
    "\n",
    "    # nombre de la configuracion\n",
    "    NAME = \"arandano_cfg_test_11_fase_3_\"    \n",
    "    \n",
    "    # clase arandano + clase background + 4 clases\n",
    "    NUM_CLASSES = 1 + 3\n",
    "    \n",
    "    # pasos por epoch y confianza minima    # STEPS_PER_EPOCH = cantidad de lotes/batchs\n",
    "    STEPS_PER_EPOCH = 61  # por epoch se entrenaran 61 lotes de 5 imagenes, dataset = 305\n",
    "\n",
    "    # tasa de aprendizaje y momentum\n",
    "    LEARNING_RATE=0.001\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # penalización de regularización\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # el tamaño de la imagen está controlado por este parámetro\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # pasos de validación\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # número de regiones de interés generadas por imagen\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # escala de anclas RPN y proporciones (ratios) para encontrar la ROI\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)    # Longitud del lado del ancla cuadrada, en píxeles \n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]   # Proporciones de anclas por cada celda (ancho/alto). Un valor de 1 representa un ancla cuadrada y 0,5 es un ancla ancha \n",
    "\n",
    "    #DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0    \n",
    "    DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "    IMAGES_PER_GPU = 2\n",
    "    \n",
    "    MINI_MASK_SHAPE = (28, 28)\n",
    "    \n",
    "ConfigArandanos().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/PT_JoseVeloso/Mask_RCNN-master_matterport\n"
     ]
    }
   ],
   "source": [
    "cd /tf/PT_JoseVeloso/Mask_RCNN-master_matterport/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Mi unidad\\\\UBB\\\\PT\\\\Mask_R-CNN\\\\Mask R-CNN\\\\entrenamiento\\\\model-training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pesos = 'arandano_cfg_test_6_fase_2_2_20220920T0813/mask_rcnn_arandano_cfg_test_6_fase_2_2__0068.h5' \n",
    "pesos = 'arandano_cfg_test_11_fase_2_2_20220921T2007/mask_rcnn_arandano_cfg_test_11_fase_2_2__0120.h5'\n",
    "pesos = 'entrenamiento/arandano_cfg_test_11_fase_2_2_20220921T2007/mask_rcnn_arandano_cfg_test_11_fase_2_2__0120.h5'\n",
    "\n",
    "conjunto_datos = 'customImages/test_11_fase_3'\n",
    "'../../../dataset_v2/test_11_fase_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3787125,
     "status": "ok",
     "timestamp": 1659604139130,
     "user": {
      "displayName": "José Ignacio Veloso Inzunza",
      "userId": "04140603416196179882"
     },
     "user_tz": 240
    },
    "id": "dF77APDH0saM",
    "outputId": "aa2a0330-e201-4b00-b12c-fee7986449cb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: ./arandano_cfg_test_11_fase_3_20221008T0712/mask_rcnn_arandano_cfg_test_11_fase_3__{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "anchors                (AnchorsLayer)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_2_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_2_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_2_grad/Cast:0\", shape=(2,), dtype=int32, device=/device:GPU:0))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_3_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_3_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_3_grad/Cast:0\", shape=(2,), dtype=int32, device=/device:GPU:0))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 603s 10s/step - batch: 30.0000 - size: 2.0000 - loss: 4.2012 - rpn_class_loss: 0.4839 - rpn_bbox_loss: 1.7585 - mrcnn_class_loss: 0.3752 - mrcnn_bbox_loss: 1.0662 - mrcnn_mask_loss: 0.5174 - val_loss: 2.9249 - val_rpn_class_loss: 0.4580 - val_rpn_bbox_loss: 0.9541 - val_mrcnn_class_loss: 0.2059 - val_mrcnn_bbox_loss: 0.7249 - val_mrcnn_mask_loss: 0.5821\n",
      "Epoch 2/300\n",
      "61/61 [==============================] - 471s 8s/step - batch: 30.0000 - size: 2.0000 - loss: 2.3464 - rpn_class_loss: 0.1057 - rpn_bbox_loss: 0.8720 - mrcnn_class_loss: 0.1681 - mrcnn_bbox_loss: 0.6606 - mrcnn_mask_loss: 0.5401 - val_loss: 2.7081 - val_rpn_class_loss: 0.4566 - val_rpn_bbox_loss: 0.8525 - val_mrcnn_class_loss: 0.2344 - val_mrcnn_bbox_loss: 0.6145 - val_mrcnn_mask_loss: 0.5501\n",
      "Epoch 3/300\n",
      "61/61 [==============================] - 428s 7s/step - batch: 30.0000 - size: 2.0000 - loss: 2.1908 - rpn_class_loss: 0.1017 - rpn_bbox_loss: 0.8278 - mrcnn_class_loss: 0.1490 - mrcnn_bbox_loss: 0.6068 - mrcnn_mask_loss: 0.5056 - val_loss: 2.6637 - val_rpn_class_loss: 0.4371 - val_rpn_bbox_loss: 0.7940 - val_mrcnn_class_loss: 0.2570 - val_mrcnn_bbox_loss: 0.6421 - val_mrcnn_mask_loss: 0.5335\n",
      "Epoch 4/300\n",
      "61/61 [==============================] - 437s 7s/step - batch: 30.0000 - size: 2.0000 - loss: 2.2138 - rpn_class_loss: 0.1123 - rpn_bbox_loss: 0.8556 - mrcnn_class_loss: 0.1569 - mrcnn_bbox_loss: 0.5842 - mrcnn_mask_loss: 0.5048 - val_loss: 2.4578 - val_rpn_class_loss: 0.3876 - val_rpn_bbox_loss: 0.7709 - val_mrcnn_class_loss: 0.1828 - val_mrcnn_bbox_loss: 0.5825 - val_mrcnn_mask_loss: 0.5340\n",
      "Epoch 5/300\n",
      "61/61 [==============================] - 390s 6s/step - batch: 30.0000 - size: 2.0000 - loss: 2.0897 - rpn_class_loss: 0.0886 - rpn_bbox_loss: 0.8183 - mrcnn_class_loss: 0.1499 - mrcnn_bbox_loss: 0.5481 - mrcnn_mask_loss: 0.4847 - val_loss: 2.6480 - val_rpn_class_loss: 0.4812 - val_rpn_bbox_loss: 0.8386 - val_mrcnn_class_loss: 0.1969 - val_mrcnn_bbox_loss: 0.5906 - val_mrcnn_mask_loss: 0.5407\n",
      "Epoch 6/300\n",
      "61/61 [==============================] - ETA: 0s - batch: 30.0000 - size: 2.0000 - loss: 2.0126 - rpn_class_loss: 0.0779 - rpn_bbox_loss: 0.7409 - mrcnn_class_loss: 0.1345 - mrcnn_bbox_loss: 0.5581 - mrcnn_mask_loss: 0.5012"
     ]
    }
   ],
   "source": [
    "# cargar dataset de entrenamiento\n",
    "train_set = DatasetArandanos()\n",
    "train_set.load_dataset(conjunto_datos, is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# cargar dataset de test \n",
    "test_set = DatasetArandanos()\n",
    "test_set.load_dataset(conjunto_datos, is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# preparar la configuración llamando a la clase de configuración definida por el usuario\n",
    "config = ConfigArandanos()\n",
    "\n",
    "# definir el modelo\n",
    "with tf.device(config.DEVICE):\n",
    "    model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# cargar pesos del modelo \n",
    "weights_path = pesos\n",
    "\n",
    "# cargar los pesos del modelo\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "#augmentation = iaa.Sometimes(0.5, iaa.Fliplr(0.5), iaa.Flipud(0.5))\n",
    "\n",
    "augmentation = iaa.Sequential([\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.2)), # random crops\n",
    "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(\n",
    "        0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    )],  \n",
    "    random_order=True)\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50, baseline=2.5, min_delta=1)\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50, min_delta=1)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    baseline=2.5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# iniciar el entrenamiento del modelo\n",
    "# layers = head or all\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=300, layers='all', augmentation=augmentation)\n",
    "#model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=300, layers='all', augmentation=augmentation, custom_callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "master.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "da38062997892a68ae88df3a1549a85ff68f4e3a875c1f51aead31b07f2af4c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
